\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dosovitskiy2015flownet}
\citation{horn1981determining}
\citation{caelles2017one}
\citation{oh2019video}
\citation{xiu2018pose}
\citation{lai2020mast}
\citation{jabri2020space}
\citation{wang2019learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{deng2009large}
\citation{deng2009large}
\citation{wang2020contrastive}
\citation{xu2021rethinking}
\citation{wang2021different}
\citation{he2020momentum}
\citation{xie2021detco}
\citation{deng2009large}
\citation{carreira2017quo}
\citation{xu2018youtube}
\citation{li2017learning}
\citation{lai2019self}
\citation{lai2020mast}
\citation{li2019joint}
\citation{vondrick2018tracking}
\citation{wang2020contrastive}
\citation{jabri2020space}
\citation{wang2019learning}
\citation{zhao2021modelling}
\citation{xu2021rethinking}
\citation{araslanov2021dense}
\citation{wang2020contrastive}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ \textbf  {Illustration of the main idea.} In (b), we first train a contrastive model on ImageNet\nobreakspace  {}\cite  {deng2009large} and fixed it as teacher. Then the distillation on global correlation map is proposed to maintain the ability to capture object appearance. In (a), the distillation is performed between local correlation maps at different pyramid levels to facilitate fine-grained matching. The local correlation map computed at lower pyramid level is regarded as pseudo labels.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tissor}{{1}{2}{\small \textbf {Illustration of the main idea.} In (b), we first train a contrastive model on ImageNet~\cite {deng2009large} and fixed it as teacher. Then the distillation on global correlation map is proposed to maintain the ability to capture object appearance. In (a), the distillation is performed between local correlation maps at different pyramid levels to facilitate fine-grained matching. The local correlation map computed at lower pyramid level is regarded as pseudo labels.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\citation{wu2018unsupervised}
\citation{he2020momentum}
\citation{caron2020unsupervised}
\citation{chen2021exploring}
\citation{grill2020bootstrap}
\citation{wang2021dense}
\citation{xie2021detco}
\citation{yang2021instance}
\citation{wang2021different}
\citation{dosovitskiy2015flownet}
\citation{teed2020raft}
\citation{meister2018unflow}
\citation{jonschkowski2020matters}
\citation{liu2020learning}
\citation{liu2019ddflow}
\citation{van2018representation}
\citation{he2020momentum}
\citation{xie2021detco}
\citation{he2020momentum}
\citation{xie2021detco}
\@writefile{toc}{\contentsline {section}{\numberline {3}Approach}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Spatial Feature Learning}{3}{subsection.3.1}}
\newlabel{spatial_feature_learning}{{3.1}{3}{Spatial Feature Learning}{subsection.3.1}{}}
\newlabel{eq:nce}{{1}{3}{Spatial Feature Learning}{equation.3.1}{}}
\citation{li2017learning}
\citation{lai2020mast}
\citation{vondrick2018tracking}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ \textbf  {An overview of our framework.} Our method decouples video correspondence learning into two separate processes, including spatial feature learning and temporal feature learning. Specifically, the spatial feature learning first exploits the contrastive loss, which is analogous to that of instance discrimination, to learn the object appearance with image data. Then we perform the self-supervised training with video data in the next step. To maintain the ability to capture object appearance, we fix the pre-trained network as teacher and a global correlation distillation is devised. For temporal feature learning, we propose a pyramid learning framework where the frame reconstruction is devised at each pyramid level of the network. At the same time, we introduce a novel loss named local correlation distillation loss that supports explicitly learning of the correlation map in the region with high uncertainty, which is achieved by taking the finest local correlation map as pseudo labels.\relax }}{4}{figure.caption.3}}
\newlabel{fig:framework}{{2}{4}{\small \textbf {An overview of our framework.} Our method decouples video correspondence learning into two separate processes, including spatial feature learning and temporal feature learning. Specifically, the spatial feature learning first exploits the contrastive loss, which is analogous to that of instance discrimination, to learn the object appearance with image data. Then we perform the self-supervised training with video data in the next step. To maintain the ability to capture object appearance, we fix the pre-trained network as teacher and a global correlation distillation is devised. For temporal feature learning, we propose a pyramid learning framework where the frame reconstruction is devised at each pyramid level of the network. At the same time, we introduce a novel loss named local correlation distillation loss that supports explicitly learning of the correlation map in the region with high uncertainty, which is achieved by taking the finest local correlation map as pseudo labels.\relax }{figure.caption.3}{}}
\newlabel{eq:global_correlation}{{2}{4}{Spatial Feature Learning}{equation.3.2}{}}
\newlabel{eq:global_correlation_loss}{{3}{4}{Spatial Feature Learning}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Temporal Feature Learning}{4}{subsection.3.2}}
\newlabel{temporal_feature_learning}{{3.2}{4}{Temporal Feature Learning}{subsection.3.2}{}}
\citation{lai2020mast}
\citation{lai2020mast}
\citation{teed2020raft}
\newlabel{eq:local_correlation}{{4}{5}{Temporal Feature Learning}{equation.3.4}{}}
\newlabel{eq:reconstruction}{{5}{5}{Temporal Feature Learning}{equation.3.5}{}}
\newlabel{eq:reconstruction loss}{{6}{5}{Temporal Feature Learning}{equation.3.6}{}}
\newlabel{eq:pyramid reconstruction loss}{{7}{5}{Temporal Feature Learning}{equation.3.7}{}}
\newlabel{eq:pyramid reconstruction loss}{{8}{5}{Temporal Feature Learning}{equation.3.8}{}}
\newlabel{eq:reconstruction loss}{{9}{5}{Temporal Feature Learning}{equation.3.9}{}}
\newlabel{eq:final loss}{{10}{5}{Temporal Feature Learning}{equation.3.10}{}}
\citation{he2016deep}
\citation{jabri2020space}
\citation{lai2020mast}
\citation{xu2021rethinking}
\citation{deng2009large}
\citation{he2020momentum}
\citation{xu2018youtube}
\citation{lai2019self}
\citation{lai2020mast}
\citation{jabri2020space}
\citation{lai2020mast}
\citation{xu2021rethinking}
\citation{pont20172017}
\citation{zhou2018adaptive}
\citation{jhuang2013towards}
\citation{pont20172017}
\citation{pont20172017}
\citation{xu2018youtube}
\citation{li2017learning}
\citation{li2017learning}
\citation{xie2021detco}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation Details}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ablation Study}{6}{subsection.4.2}}
\citation{li2017learning}
\citation{deng2009large}
\citation{xu2018youtube}
\citation{deng2009large}
\citation{xu2018youtube}
\citation{jabri2020space}
\citation{lai2020mast}
\citation{xu2021rethinking}
\citation{jabri2020space}
\citation{lai2020mast}
\citation{xu2021rethinking}
\citation{miao2021self}
\citation{he2016deep}
\citation{wang2019learning}
\citation{li2019joint}
\citation{jabri2020space}
\citation{wang2020contrastive}
\citation{xu2021rethinking}
\citation{jeon2021mining}
\citation{zhao2021modelling}
\citation{zhou2018adaptive}
\citation{song2017thin}
\newlabel{table:ablation2}{{5a}{7}{Subtable 5a}{subtable.5.1}{}}
\newlabel{sub@table:ablation2}{{(a)}{a}{Subtable 5a\relax }{subtable.5.1}{}}
\newlabel{table:ablation2}{{5b}{7}{Subtable 5b}{subtable.5.2}{}}
\newlabel{sub@table:ablation2}{{(b)}{b}{Subtable 5b\relax }{subtable.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \textbf  {Ablation study for each component in our framework}. The "$p$" and "$e$" in (a) correspond to pyramid frame reconstruction and entropy-based selection. Models in (b) are all pre-trained on ImageNet with contrastive loss and models with "w" are subsequently trained on YouTube-VOS using different methods. I: ImageNet\nobreakspace  {}\cite  {deng2009large}. YTV: YouTube-VOS\nobreakspace  {}\cite  {xu2018youtube}.\relax }}{7}{table.caption.4}}
\newlabel{tab:ablations}{{5}{7}{\textbf {Ablation study for each component in our framework}. The "$p$" and "$e$" in (a) correspond to pyramid frame reconstruction and entropy-based selection. Models in (b) are all pre-trained on ImageNet with contrastive loss and models with "w" are subsequently trained on YouTube-VOS using different methods. I: ImageNet~\cite {deng2009large}. YTV: YouTube-VOS~\cite {xu2018youtube}.\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {{Ablation study of each component.} }}}{7}{subtable.1.1}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {{Ablation study of $\mathcal {L}_{\mathrm {gc}}$.} }}}{7}{subtable.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ \textbf  {Visualization of the ablation study.} Given a query point randomly sampled in the target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. The dashed line in red represents the range of computing correlation map $w.r.t.$ query point. The reference frame is randomly sampled in the memory bank of inference strategy\nobreakspace  {}\cite  {jabri2020space}\cite  {lai2020mast}\cite  {xu2021rethinking}. \relax }}{7}{figure.caption.5}}
\newlabel{fig:ablations}{{3}{7}{\small \textbf {Visualization of the ablation study.} Given a query point randomly sampled in the target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. The dashed line in red represents the range of computing correlation map $w.r.t.$ query point. The reference frame is randomly sampled in the memory bank of inference strategy~\cite {jabri2020space}\cite {lai2020mast}\cite {xu2021rethinking}. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with State-of-the-art}{7}{subsection.4.3}}
\citation{carreira2017quo}
\citation{muller2018trackingnet}
\citation{xu2018youtube}
\citation{xu2021rethinking}
\citation{wang2019fast}
\citation{voigtlaender2017online}
\citation{maninis2018video}
\citation{he2016deep}
\citation{he2020momentum}
\citation{chen2021exploring}
\citation{vondrick2018tracking}
\citation{lai2019self}
\citation{lu2020learning}
\citation{li2019joint}
\citation{wang2020contrastive}
\citation{xu2021rethinking}
\citation{jabri2020space}
\citation{zhao2021modelling}
\citation{araslanov2021dense}
\citation{lai2020mast}
\citation{miao2021self}
\citation{he2016deep}
\citation{he2020momentum}
\citation{chen2021exploring}
\citation{wang2019learning}
\citation{li2019joint}
\citation{yao2021seco}
\citation{gordon2020watching}
\citation{xu2021rethinking}
\citation{wang2019fast}
\citation{voigtlaender2017online}
\citation{maninis2018video}
\citation{pont20172017}
\citation{deng2009large}
\citation{lin2014microsoft}
\citation{valmadre2018long}
\citation{muller2018trackingnet}
\citation{carreira2017quo}
\citation{fouhey2018lifestyle}
\citation{xu2018youtube}
\citation{pont20172017}
\citation{everingham2015pascal}
\citation{pont20172017}
\citation{deng2009large}
\citation{lin2014microsoft}
\citation{valmadre2018long}
\citation{muller2018trackingnet}
\citation{carreira2017quo}
\citation{fouhey2018lifestyle}
\citation{xu2018youtube}
\citation{pont20172017}
\citation{everingham2015pascal}
\citation{zhou2018adaptive}
\citation{pont20172017}
\citation{zhou2018adaptive}
\citation{wang2019learning}
\citation{zhou2018adaptive}
\citation{jhuang2013towards}
\citation{yang2012articulated}
\citation{jabri2020space}
\citation{li2019joint}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Quantitative results for video object segmentation on validation set of DAVIS-2017}\nobreakspace  {}\cite  {pont20172017}. We show results of state-of-the-art self-supervised methods and some \leavevmode {\color  {gray}supervised} methods for comparison. "Dataset" represents the dataset(s) for pre-training, including: \nobreakspace  {}I:ImageNet\nobreakspace  {}\cite  {deng2009large}\nobreakspace  {}(1.28m). \nobreakspace  {}C:COCO\nobreakspace  {}\cite  {lin2014microsoft}\nobreakspace  {}(30k). \nobreakspace  {}O:OxUvA\nobreakspace  {}\cite  {valmadre2018long}\nobreakspace  {}(14h). \nobreakspace  {}T:TrackingNet\nobreakspace  {}\cite  {muller2018trackingnet}\nobreakspace  {}(300h). \nobreakspace  {}K:Kinetics\nobreakspace  {}\cite  {carreira2017quo}\nobreakspace  {}(800h). \nobreakspace  {}V:VLOG\nobreakspace  {}\cite  {fouhey2018lifestyle}\nobreakspace  {}(344h). \nobreakspace  {}YTV:YouTube-VOS\nobreakspace  {}\cite  {xu2018youtube}\nobreakspace  {}(5h). \nobreakspace  {}D:DAVIS-2017\nobreakspace  {}\cite  {pont20172017}\nobreakspace  {}(-). \nobreakspace  {}P:PASCAL-VOC\nobreakspace  {}\cite  {everingham2015pascal}\nobreakspace  {}(-). We report the data size for self-supervised methods\nobreakspace  {}(\nobreakspace  {}total number/duration of image/video dataset\nobreakspace  {}).\relax }}{8}{table.caption.7}}
\newlabel{table:sota}{{3}{8}{\textbf {Quantitative results for video object segmentation on validation set of DAVIS-2017}~\cite {pont20172017}. We show results of state-of-the-art self-supervised methods and some \textcolor {gray}{supervised} methods for comparison. "Dataset" represents the dataset(s) for pre-training, including: ~I:ImageNet~\cite {deng2009large}~(1.28m). ~C:COCO~\cite {lin2014microsoft}~(30k). ~O:OxUvA~\cite {valmadre2018long}~(14h). ~T:TrackingNet~\cite {muller2018trackingnet}~(300h). ~K:Kinetics~\cite {carreira2017quo}~(800h). ~V:VLOG~\cite {fouhey2018lifestyle}~(344h). ~YTV:YouTube-VOS~\cite {xu2018youtube}~(5h). ~D:DAVIS-2017~\cite {pont20172017}~(-). ~P:PASCAL-VOC~\cite {everingham2015pascal}~(-). We report the data size for self-supervised methods~(~total number/duration of image/video dataset~).\relax }{table.caption.7}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Quantitative results for human part propagation and human pose tracking.} We show results of state-of-the-art self-supervised methods and some \leavevmode {\color  {gray}supervised} methods for comparison.\relax }}{8}{table.caption.6}}
\newlabel{table:vip}{{2}{8}{\textbf {Quantitative results for human part propagation and human pose tracking.} We show results of state-of-the-art self-supervised methods and some \textcolor {gray}{supervised} methods for comparison.\relax }{table.caption.6}{}}
\citation{pont20172017}
\citation{zhou2018adaptive}
\citation{jhuang2013towards}
\citation{pont20172017}
\citation{zhou2018adaptive}
\citation{jhuang2013towards}
\bibstyle{abbrvnat}
\bibdata{neurips_2022}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Qualitative results for label propagation. Given the first frame with different annotations highlighted with a blue outline, we propagate it to the current frame without fine-tuning. (a) Video object segmentation on DAVIS-2017\nobreakspace  {}\cite  {pont20172017}. (b) Human part propagation on VIP\nobreakspace  {}\cite  {zhou2018adaptive}. (c) Pose keypoint tracking on JHMDB\nobreakspace  {}\cite  {jhuang2013towards}. \relax }}{9}{figure.caption.8}}
\newlabel{fig:quan}{{4}{9}{\small Qualitative results for label propagation. Given the first frame with different annotations highlighted with a blue outline, we propagate it to the current frame without fine-tuning. (a) Video object segmentation on DAVIS-2017~\cite {pont20172017}. (b) Human part propagation on VIP~\cite {zhou2018adaptive}. (c) Pose keypoint tracking on JHMDB~\cite {jhuang2013towards}. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{9}{section.5}}
\bibcite{araslanov2021dense}{{1}{2021}{{Araslanov et~al.}}{{Araslanov, Schaub-Meyer, and Roth}}}
\bibcite{caelles2017one}{{2}{2017}{{Caelles et~al.}}{{Caelles, Maninis, Pont-Tuset, Leal-Taix{\'e}, Cremers, and Van~Gool}}}
\bibcite{caron2020unsupervised}{{3}{2020}{{Caron et~al.}}{{Caron, Misra, Mairal, Goyal, Bojanowski, and Joulin}}}
\bibcite{carreira2017quo}{{4}{2017}{{Carreira and Zisserman}}{{}}}
\bibcite{chen2021exploring}{{5}{2021}{{Chen and He}}{{}}}
\bibcite{deng2009large}{{6}{2009}{{Deng}}{{}}}
\bibcite{dosovitskiy2015flownet}{{7}{2015}{{Dosovitskiy et~al.}}{{Dosovitskiy, Fischer, Ilg, Hausser, Hazirbas, Golkov, Van Der~Smagt, Cremers, and Brox}}}
\bibcite{everingham2015pascal}{{8}{2015}{{Everingham et~al.}}{{Everingham, Eslami, Van~Gool, Williams, Winn, and Zisserman}}}
\bibcite{fouhey2018lifestyle}{{9}{2018}{{Fouhey et~al.}}{{Fouhey, Kuo, Efros, and Malik}}}
\bibcite{gordon2020watching}{{10}{2020}{{Gordon et~al.}}{{Gordon, Ehsani, Fox, and Farhadi}}}
\bibcite{grill2020bootstrap}{{11}{2020}{{Grill et~al.}}{{Grill, Strub, Altch{\'e}, Tallec, Richemond, Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar, et~al.}}}
\bibcite{he2016deep}{{12}{2016}{{He et~al.}}{{He, Zhang, Ren, and Sun}}}
\bibcite{he2020momentum}{{13}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{horn1981determining}{{14}{1981}{{Horn and Schunck}}{{}}}
\bibcite{jabri2020space}{{15}{2020}{{Jabri et~al.}}{{Jabri, Owens, and Efros}}}
\bibcite{jeon2021mining}{{16}{2021}{{Jeon et~al.}}{{Jeon, Min, Kim, and Sohn}}}
\bibcite{jhuang2013towards}{{17}{2013}{{Jhuang et~al.}}{{Jhuang, Gall, Zuffi, Schmid, and Black}}}
\bibcite{jonschkowski2020matters}{{18}{2020}{{Jonschkowski et~al.}}{{Jonschkowski, Stone, Barron, Gordon, Konolige, and Angelova}}}
\bibcite{lai2019self}{{19}{2019}{{Lai and Xie}}{{}}}
\bibcite{lai2020mast}{{20}{2020}{{Lai et~al.}}{{Lai, Lu, and Xie}}}
\bibcite{li2019joint}{{21}{2019}{{Li et~al.}}{{Li, Liu, De~Mello, Wang, Kautz, and Yang}}}
\bibcite{li2017learning}{{22}{2017}{{Li and Hoiem}}{{}}}
\bibcite{lin2014microsoft}{{23}{2014}{{Lin et~al.}}{{Lin, Maire, Belongie, Hays, Perona, Ramanan, Doll{\'a}r, and Zitnick}}}
\bibcite{liu2020learning}{{24}{2020}{{Liu et~al.}}{{Liu, Zhang, He, Liu, Wang, Tai, Luo, Wang, Li, and Huang}}}
\bibcite{liu2019ddflow}{{25}{2019}{{Liu et~al.}}{{Liu, King, Lyu, and Xu}}}
\bibcite{lu2020learning}{{26}{2020}{{Lu et~al.}}{{Lu, Wang, Shen, Tai, Crandall, and Hoi}}}
\bibcite{maninis2018video}{{27}{2018}{{Maninis et~al.}}{{Maninis, Caelles, Chen, Pont-Tuset, Leal-Taix{\'e}, Cremers, and Van~Gool}}}
\bibcite{meister2018unflow}{{28}{2018}{{Meister et~al.}}{{Meister, Hur, and Roth}}}
\bibcite{miao2021self}{{29}{2021}{{Miao et~al.}}{{Miao, Bennamoun, Gao, and Mian}}}
\bibcite{muller2018trackingnet}{{30}{2018}{{Muller et~al.}}{{Muller, Bibi, Giancola, Alsubaihi, and Ghanem}}}
\bibcite{oh2019video}{{31}{2019}{{Oh et~al.}}{{Oh, Lee, Xu, and Kim}}}
\bibcite{pont20172017}{{32}{2017}{{Pont-Tuset et~al.}}{{Pont-Tuset, Perazzi, Caelles, Arbel{\'a}ez, Sorkine-Hornung, and Van~Gool}}}
\bibcite{song2017thin}{{33}{2017}{{Song et~al.}}{{Song, Wang, Van~Gool, and Hilliges}}}
\bibcite{teed2020raft}{{34}{2020}{{Teed and Deng}}{{}}}
\bibcite{valmadre2018long}{{35}{2018}{{Valmadre et~al.}}{{Valmadre, Bertinetto, Henriques, Tao, Vedaldi, Smeulders, Torr, and Gavves}}}
\bibcite{van2018representation}{{36}{2018}{{Van~den Oord et~al.}}{{Van~den Oord, Li, and Vinyals}}}
\bibcite{voigtlaender2017online}{{37}{2017}{{Voigtlaender and Leibe}}{{}}}
\bibcite{vondrick2018tracking}{{38}{2018}{{Vondrick et~al.}}{{Vondrick, Shrivastava, Fathi, Guadarrama, and Murphy}}}
\bibcite{wang2020contrastive}{{39}{2020}{{Wang et~al.}}{{Wang, Zhou, and Li}}}
\bibcite{wang2019fast}{{40}{2019{}}{{Wang et~al.}}{{Wang, Zhang, Bertinetto, Hu, and Torr}}}
\bibcite{wang2019learning}{{41}{2019{}}{{Wang et~al.}}{{Wang, Jabri, and Efros}}}
\bibcite{wang2021dense}{{42}{2021{}}{{Wang et~al.}}{{Wang, Zhang, Shen, Kong, and Li}}}
\bibcite{wang2021different}{{43}{2021{}}{{Wang et~al.}}{{Wang, Zhao, Li, Wang, Torr, and Bertinetto}}}
\bibcite{wu2018unsupervised}{{44}{2018}{{Wu et~al.}}{{Wu, Xiong, Yu, and Lin}}}
\bibcite{xie2021detco}{{45}{2021}{{Xie et~al.}}{{Xie, Ding, Wang, Zhan, Xu, Sun, Li, and Luo}}}
\bibcite{xiu2018pose}{{46}{2018}{{Xiu et~al.}}{{Xiu, Li, Wang, Fang, and Lu}}}
\bibcite{xu2021rethinking}{{47}{2021}{{Xu and Wang}}{{}}}
\bibcite{xu2018youtube}{{48}{2018}{{Xu et~al.}}{{Xu, Yang, Fan, Yue, Liang, Yang, and Huang}}}
\bibcite{yang2021instance}{{49}{2021}{{Yang et~al.}}{{Yang, Wu, Zhou, and Lin}}}
\bibcite{yang2012articulated}{{50}{2012}{{Yang and Ramanan}}{{}}}
\bibcite{yao2021seco}{{51}{2021}{{Yao et~al.}}{{Yao, Zhang, Qiu, Pan, and Mei}}}
\bibcite{zhao2021modelling}{{52}{2021}{{Zhao et~al.}}{{Zhao, Jin, and Heng}}}
\bibcite{zhou2018adaptive}{{53}{2018}{{Zhou et~al.}}{{Zhou, Liang, Gong, and Lin}}}
