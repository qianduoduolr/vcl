\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{dosovitskiy2015flownet}
\citation{horn1981determining}
\citation{caelles2017one}
\citation{oh2019video}
\citation{xiu2018pose}
\citation{lai2020mast}
\citation{jabri2020space}
\citation{wang2019learning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{wang2020contrastive}
\citation{xu2021rethinking}
\citation{wang2021different}
\citation{he2020momentum}
\citation{xie2021detco}
\citation{deng2009large}
\citation{carreira2017quo}
\citation{xu2018youtube}
\citation{li2017learning}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Visualization of our key idea. Given a query point randomly sampled in target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. The dashed line in red represents the range of computing correlation map $w.r.t.$ query point. The reference frame is randomly sampled in the memory bank of inference strategy [11]. \relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tissor}{{1}{2}{\small Visualization of our key idea. Given a query point randomly sampled in target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. The dashed line in red represents the range of computing correlation map $w.r.t.$ query point. The reference frame is randomly sampled in the memory bank of inference strategy [11]. \relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{3}{section.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Approach}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Spatial Feature Learning}{3}{subsection.3.1}}
\newlabel{spatial_feature_learning}{{3.1}{3}{Spatial Feature Learning}{subsection.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ An overview of our spatial temporal feature learning framework. Our method decouples video correspondence learning into two separate processes including spatial feature learning and temporal feature learning. Specifically, the \textbf  {spatial feature learning} first exploits the contrastive loss which is analogous to that of instance discrimination to learn the object appearance with image data. Then we perform the self-supervised training with video data in the next step. To maintain the ability to capture object appearance, we fix the pre-trained network as teacher and a global correlation distillation is devised. For \textbf  {temporal feature learning}, we propose a pyramid learning framework where the frame reconstruction is devised at each levels of network. As the same time, we introduce a novel loss named local correlation distillation loss that supports explicitly learning of the correlation map at the region with high uncertainty, which is achieved by taking the finest local correlation map as pseudo labels.\relax }}{4}{figure.caption.3}}
\newlabel{fig:framework}{{2}{4}{\small An overview of our spatial temporal feature learning framework. Our method decouples video correspondence learning into two separate processes including spatial feature learning and temporal feature learning. Specifically, the \textbf {spatial feature learning} first exploits the contrastive loss which is analogous to that of instance discrimination to learn the object appearance with image data. Then we perform the self-supervised training with video data in the next step. To maintain the ability to capture object appearance, we fix the pre-trained network as teacher and a global correlation distillation is devised. For \textbf {temporal feature learning}, we propose a pyramid learning framework where the frame reconstruction is devised at each levels of network. As the same time, we introduce a novel loss named local correlation distillation loss that supports explicitly learning of the correlation map at the region with high uncertainty, which is achieved by taking the finest local correlation map as pseudo labels.\relax }{figure.caption.3}{}}
\newlabel{eq:nce}{{1}{4}{Spatial Feature Learning}{equation.3.1}{}}
\newlabel{eq:global_correlation}{{2}{4}{Spatial Feature Learning}{equation.3.2}{}}
\newlabel{eq:global_correlation_loss}{{3}{4}{Spatial Feature Learning}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Temporal Feature Learning}{5}{subsection.3.2}}
\newlabel{temporal_feature_learning}{{3.2}{5}{Temporal Feature Learning}{subsection.3.2}{}}
\newlabel{eq:local_correlation}{{4}{5}{Temporal Feature Learning}{equation.3.4}{}}
\newlabel{eq:reconstruction}{{5}{5}{Temporal Feature Learning}{equation.3.5}{}}
\newlabel{eq:reconstruction loss}{{6}{5}{Temporal Feature Learning}{equation.3.6}{}}
\newlabel{eq:pyramid reconstruction loss}{{7}{5}{Temporal Feature Learning}{equation.3.7}{}}
\newlabel{eq:pyramid reconstruction loss}{{8}{5}{Temporal Feature Learning}{equation.3.8}{}}
\newlabel{eq:reconstruction loss}{{9}{6}{Temporal Feature Learning}{equation.3.9}{}}
\newlabel{eq:final loss}{{10}{6}{Temporal Feature Learning}{equation.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{6}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation Details}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ablation Study}{6}{subsection.4.2}}
\newlabel{table:ablation2}{{5a}{7}{Subtable 5a}{subtable.5.1}{}}
\newlabel{sub@table:ablation2}{{(a)}{a}{Subtable 5a\relax }{subtable.5.1}{}}
\newlabel{table:ablation2}{{5b}{7}{Subtable 5b}{subtable.5.2}{}}
\newlabel{sub@table:ablation2}{{(b)}{b}{Subtable 5b\relax }{subtable.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Ablation study for each component in spatial and temporal feature learning. The "$p$" and "$e$" in (a) correspond to pyramid frame reconstruction and entropy-based selection. Models in (b) are all pre-trained on ImageNet with contrastive loss and models with "w" are subsequently trained on YouTube-VOS using different methods. I: ImageNet [11]. YTV: YouTube-VOS [12].\relax }}{7}{table.caption.4}}
\newlabel{tab:ablations}{{5}{7}{Ablation study for each component in spatial and temporal feature learning. The "$p$" and "$e$" in (a) correspond to pyramid frame reconstruction and entropy-based selection. Models in (b) are all pre-trained on ImageNet with contrastive loss and models with "w" are subsequently trained on YouTube-VOS using different methods. I: ImageNet [11]. YTV: YouTube-VOS [12].\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {{Ablation study of each component.} }}}{7}{subtable.1.1}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {{Ablation study of $\mathcal {L}_{\mathrm {gc}}$.} }}}{7}{subtable.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Visualization of the ablation study. Given a query point randomly sampled in target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. The dashed line in red represents the range of computing correlation map $w.r.t.$ query point. The reference frame is randomly sampled in the memory bank of inference strategy [11]. \relax }}{7}{figure.caption.5}}
\newlabel{fig:ablations}{{3}{7}{\small Visualization of the ablation study. Given a query point randomly sampled in target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. The dashed line in red represents the range of computing correlation map $w.r.t.$ query point. The reference frame is randomly sampled in the memory bank of inference strategy [11]. \relax }{figure.caption.5}{}}
\citation{wang2019learning}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Quantitative results for video object segmentation on validation set of DAVIS-2017}. We show results of state-of-the-art self-supervised methods and some \leavevmode {\color  {gray}supervised} methods for comparison. "Dataset" represents the dataset(s) for pre-training, including: \nobreakspace  {}I:ImageNet\nobreakspace  {}(1.28m). \nobreakspace  {}C:COCO\nobreakspace  {}(30k). \nobreakspace  {}O:OxUvA\nobreakspace  {}(14h). \nobreakspace  {}T:TrackingNet\nobreakspace  {}(300h). \nobreakspace  {}K:Kinetics\nobreakspace  {}(800h). \nobreakspace  {}V:VLOG\nobreakspace  {}(344h). \nobreakspace  {}YTV:YouTube-VOS\nobreakspace  {}(5h). \nobreakspace  {}D:DAVIS 2017\nobreakspace  {}(-). \nobreakspace  {}P:PASCAL-VOC\nobreakspace  {}(-). We report the data size for self-supervised methods\nobreakspace  {}(\nobreakspace  {}total number/duration of image/video dataset\nobreakspace  {}).\relax }}{8}{table.caption.6}}
\newlabel{table:sota}{{2}{8}{\textbf {Quantitative results for video object segmentation on validation set of DAVIS-2017}. We show results of state-of-the-art self-supervised methods and some \textcolor {gray}{supervised} methods for comparison. "Dataset" represents the dataset(s) for pre-training, including: ~I:ImageNet~(1.28m). ~C:COCO~(30k). ~O:OxUvA~(14h). ~T:TrackingNet~(300h). ~K:Kinetics~(800h). ~V:VLOG~(344h). ~YTV:YouTube-VOS~(5h). ~D:DAVIS 2017~(-). ~P:PASCAL-VOC~(-). We report the data size for self-supervised methods~(~total number/duration of image/video dataset~).\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with State-of-the-art}{8}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Quantitative results for human part propagation and human pose tracking.} We show results of state-of-the-art self-supervised methods and some \leavevmode {\color  {gray}supervised} methods for comparison.\relax }}{8}{table.caption.7}}
\newlabel{table:vip}{{3}{8}{\textbf {Quantitative results for human part propagation and human pose tracking.} We show results of state-of-the-art self-supervised methods and some \textcolor {gray}{supervised} methods for comparison.\relax }{table.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Qualitative results for label propagation. Given a first frame with different annotation highlighted with a blue outline, we propagate it to current frame without fine-tuning. (a) Video object segmentation on DAVIS-2017. (b) Human part propagation on VIP. (c) Pose keypoint tracking on JHMDB. \relax }}{9}{figure.caption.8}}
\newlabel{fig:quan}{{4}{9}{\small Qualitative results for label propagation. Given a first frame with different annotation highlighted with a blue outline, we propagate it to current frame without fine-tuning. (a) Video object segmentation on DAVIS-2017. (b) Human part propagation on VIP. (c) Pose keypoint tracking on JHMDB. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{9}{section.5}}
\bibstyle{abbrvnat}
\bibdata{neurips_2022}
\bibcite{caelles2017one}{{1}{2017}{{Caelles et~al.}}{{Caelles, Maninis, Pont-Tuset, Leal-Taix{\'e}, Cremers, and Van~Gool}}}
\bibcite{carreira2017quo}{{2}{2017}{{Carreira and Zisserman}}{{}}}
\bibcite{deng2009large}{{3}{2009}{{Deng}}{{}}}
\bibcite{dosovitskiy2015flownet}{{4}{2015}{{Dosovitskiy et~al.}}{{Dosovitskiy, Fischer, Ilg, Hausser, Hazirbas, Golkov, Van Der~Smagt, Cremers, and Brox}}}
\bibcite{he2020momentum}{{5}{2020}{{He et~al.}}{{He, Fan, Wu, Xie, and Girshick}}}
\bibcite{horn1981determining}{{6}{1981}{{Horn and Schunck}}{{}}}
\bibcite{jabri2020space}{{7}{2020}{{Jabri et~al.}}{{Jabri, Owens, and Efros}}}
\bibcite{lai2020mast}{{8}{2020}{{Lai et~al.}}{{Lai, Lu, and Xie}}}
\bibcite{li2017learning}{{9}{2017}{{Li and Hoiem}}{{}}}
\bibcite{oh2019video}{{10}{2019}{{Oh et~al.}}{{Oh, Lee, Xu, and Kim}}}
\bibcite{wang2020contrastive}{{11}{2020}{{Wang et~al.}}{{Wang, Zhou, and Li}}}
\bibcite{wang2019learning}{{12}{2019}{{Wang et~al.}}{{Wang, Jabri, and Efros}}}
\bibcite{wang2021different}{{13}{2021}{{Wang et~al.}}{{Wang, Zhao, Li, Wang, Torr, and Bertinetto}}}
\bibcite{xie2021detco}{{14}{2021}{{Xie et~al.}}{{Xie, Ding, Wang, Zhan, Xu, Sun, Li, and Luo}}}
\bibcite{xiu2018pose}{{15}{2018}{{Xiu et~al.}}{{Xiu, Li, Wang, Fang, and Lu}}}
\bibcite{xu2021rethinking}{{16}{2021}{{Xu and Wang}}{{}}}
\bibcite{xu2018youtube}{{17}{2018}{{Xu et~al.}}{{Xu, Yang, Fan, Yue, Liang, Yang, and Huang}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{11}{appendix.A}}
