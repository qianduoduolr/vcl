\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ tissor.\relax }}{2}{figure.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:tissor}{{1}{2}{\small tissor.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{2}{section.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ An overview of our spatial temporal feature learning framework. Our method decouples video correspondence learning into two separate processes including spatial feature learning and temporal feature learning. Specifically, the \textbf  {spatial feature learning} first exploits the contrastive loss which is analogous to that of instance discrimination to learn the object appearance with image data. Then we perform the self-supervised training with video data in the next step. To maintain the ability to capture object appearance, we fix the pre-trained network as teacher and a global correlation distillation is devised. For \textbf  {temporal feature learning}, we propose a pyramid learning framework where the frame reconstruction is devised at each levels of network. As the same time, we introduce a novel loss named local correlation distillation loss that supports explicitly learning of the correlation map at the region with high uncertainty, which is achieved by taking the finest local correlation map as pseudo labels.\relax }}{3}{figure.caption.3}}
\newlabel{fig:framework}{{2}{3}{\small An overview of our spatial temporal feature learning framework. Our method decouples video correspondence learning into two separate processes including spatial feature learning and temporal feature learning. Specifically, the \textbf {spatial feature learning} first exploits the contrastive loss which is analogous to that of instance discrimination to learn the object appearance with image data. Then we perform the self-supervised training with video data in the next step. To maintain the ability to capture object appearance, we fix the pre-trained network as teacher and a global correlation distillation is devised. For \textbf {temporal feature learning}, we propose a pyramid learning framework where the frame reconstruction is devised at each levels of network. As the same time, we introduce a novel loss named local correlation distillation loss that supports explicitly learning of the correlation map at the region with high uncertainty, which is achieved by taking the finest local correlation map as pseudo labels.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Approach}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Spatial Feature Learning}{3}{subsection.3.1}}
\newlabel{spatial_feature_learning}{{3.1}{3}{Spatial Feature Learning}{subsection.3.1}{}}
\newlabel{eq:nce}{{1}{4}{Spatial Feature Learning}{equation.3.1}{}}
\newlabel{eq:global_correlation}{{2}{4}{Spatial Feature Learning}{equation.3.2}{}}
\newlabel{eq:global_correlation_loss}{{3}{4}{Spatial Feature Learning}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Temporal Feature Learning}{4}{subsection.3.2}}
\newlabel{temporal_feature_learning}{{3.2}{4}{Temporal Feature Learning}{subsection.3.2}{}}
\newlabel{eq:local_correlation}{{4}{4}{Temporal Feature Learning}{equation.3.4}{}}
\newlabel{eq:reconstruction}{{5}{4}{Temporal Feature Learning}{equation.3.5}{}}
\newlabel{eq:reconstruction loss}{{6}{5}{Temporal Feature Learning}{equation.3.6}{}}
\newlabel{eq:pyramid reconstruction loss}{{7}{5}{Temporal Feature Learning}{equation.3.7}{}}
\newlabel{eq:pyramid reconstruction loss}{{8}{5}{Temporal Feature Learning}{equation.3.8}{}}
\newlabel{eq:reconstruction loss}{{9}{5}{Temporal Feature Learning}{equation.3.9}{}}
\newlabel{eq:final loss}{{10}{5}{Temporal Feature Learning}{equation.3.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experiments}{5}{section.4}}
\newlabel{table:ablation2}{{5a}{6}{Subtable 5a}{subtable.5.1}{}}
\newlabel{sub@table:ablation2}{{(a)}{a}{Subtable 5a\relax }{subtable.5.1}{}}
\newlabel{table:ablation2}{{5b}{6}{Subtable 5b}{subtable.5.2}{}}
\newlabel{sub@table:ablation2}{{(b)}{b}{Subtable 5b\relax }{subtable.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces Ablation study for each component in spatial and temporal feature learning. The "+" in (b) represents the method used in additional to $\mathcal  {L}_{\mathrm  {t}}$ after trained with contrastive loss. I: ImageNet [11]. YTV: YouTube-VOS [12].\relax }}{6}{table.caption.4}}
\newlabel{tab:ablations}{{5}{6}{Ablation study for each component in spatial and temporal feature learning. The "+" in (b) represents the method used in additional to $\mathcal {L}_{\mathrm {t}}$ after trained with contrastive loss. I: ImageNet [11]. YTV: YouTube-VOS [12].\relax }{table.caption.4}{}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(a)}{\ignorespaces {{Ablation study of each component.} }}}{6}{subtable.1.1}}
\@writefile{lot}{\contentsline {subtable}{\numberline{(b)}{\ignorespaces {{Ablation study of $L_{\mathrm {gc}}$.} }}}{6}{subtable.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Implementation Details}{6}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Ablation Study}{6}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \relax \fontsize  {9}{10}\selectfont  \abovedisplayskip 6\p@ plus1.5\p@ minus4\p@ \abovedisplayshortskip \z@ plus2\p@ \belowdisplayskip \abovedisplayskip \belowdisplayshortskip 3\p@ plus2\p@ minus2\p@ Visualization of the ablation study. Given a query point randomly sampled in target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. Here "$\mathcal  {L}_{\mathrm  {nce}}$", "$\mathcal  {L}_{\mathrm  {t}}$" and "all" corresponds to contrastive loss of spatial feature learning, the loss of temporal feature learning and our full model. \relax }}{7}{figure.caption.5}}
\newlabel{fig:ablations}{{3}{7}{\small Visualization of the ablation study. Given a query point randomly sampled in target frame, we visualize the result of computing the local correlation and global correlation map $w.r.t.$ reference frame. Here "$\mathcal {L}_{\mathrm {nce}}$", "$\mathcal {L}_{\mathrm {t}}$" and "all" corresponds to contrastive loss of spatial feature learning, the loss of temporal feature learning and our full model. \relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Comparison with State-of-the-art}{7}{subsection.4.3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Quantitative results for video object segmentation}. I: ImageNet. YTV: YouTube-VOS. C: COCO. T: TrackingNet.\relax }}{8}{table.caption.6}}
\newlabel{table:davis}{{2}{8}{\textbf {Quantitative results for video object segmentation}. I: ImageNet. YTV: YouTube-VOS. C: COCO. T: TrackingNet.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions}{8}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{9}{appendix.A}}
