{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (/home/lr/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:301742)",
      "at S.execute (/home/lr/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:300732)",
      "at S.start (/home/lr/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:296408)",
      "at async t.CellExecutionQueue.executeQueuedCells (/home/lr/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (/home/lr/.vscode-server/extensions/ms-toolsai.jupyter-2021.10.1101450599/out/client/extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "import _init_paths\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dall_e  import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "from vcl.models.components.vqvae import VQVAE\n",
    "from vcl.models.vqvae import VQCL\n",
    "from mmcv.runner import get_dist_info, init_dist, load_checkpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "target_image_size = 256\n",
    "\n",
    "def preprocess(img):\n",
    "    s = min(img.size)\n",
    "    \n",
    "    if s < target_image_size:\n",
    "        raise ValueError(f'min dim for image {s} < {target_image_size}')\n",
    "        \n",
    "    r = target_image_size / s\n",
    "    s = (round(r * img.size[1]), round(r * img.size[0]))\n",
    "    img = TF.resize(img, s, interpolation=Image.LANCZOS)\n",
    "    img = TF.center_crop(img, output_size=2 * [target_image_size])\n",
    "    img = torch.unsqueeze(T.ToTensor()(img), 0)\n",
    "    return map_pixels(img)\n",
    "\n",
    "def load_anno():\n",
    "    video_dir = '/home/lr/dataset/YouTube-VOS/2018/train/JPEGImages'\n",
    "    list_path = '/home/lr/dataset/YouTube-VOS/2018/train/youtube2018_train_list.txt'\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    with open(list_path, 'r') as f:\n",
    "        for idx, line in enumerate(f.readlines()):\n",
    "            sample = dict()\n",
    "            vname, num_frames = line.strip('\\n').split()\n",
    "            sample['frames_path'] = sorted(glob.glob(osp.join(video_dir, vname, '*.jpg')))\n",
    "            sample['num_frames'] = int(num_frames)\n",
    "            samples.append(sample)\n",
    "    return samples\n",
    "\n",
    "def visualize_vqvae(z1_q, z2_q, frame1, frame2, x_rec1, x_rec2, nembed=2048, rescale=True):\n",
    "    frame_vq_1 = z1_q.permute(1,2,0).numpy()\n",
    "    frame_vq_2 = z2_q.permute(1,2,0).numpy()\n",
    "\n",
    "    print(len(np.unique(frame_vq_1)))\n",
    "\n",
    "    if rescale:\n",
    "        frame_vq_1 = (frame_vq_1 * 255 / nembed).astype(np.uint8)\n",
    "        frame_vq_2 = (frame_vq_2 * 255 / nembed).astype(np.uint8)\n",
    "    else:\n",
    "        frame_vq_1 = (frame_vq_1).astype(np.uint8)\n",
    "        frame_vq_2 = (frame_vq_2).astype(np.uint8)\n",
    "\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "    plt.figure()\n",
    "\n",
    "    if x_rec1 is not None:\n",
    "        plt.subplot(3,2,1)\n",
    "        plt.imshow(frame_vq_1, cmap=plt.get_cmap('jet'))\n",
    "        plt.subplot(3,2,2)\n",
    "        plt.imshow(frame_vq_2, cmap=plt.get_cmap('jet'))\n",
    "\n",
    "        plt.subplot(3,2,3)\n",
    "        plt.imshow(np.array(frame1))\n",
    "\n",
    "        plt.subplot(3,2,4)\n",
    "        plt.imshow(np.array(frame2))\n",
    "\n",
    "        plt.subplot(3,2,5)\n",
    "        plt.imshow(np.array(x_rec1))\n",
    "\n",
    "        plt.subplot(3,2,6)\n",
    "        plt.imshow(np.array(x_rec2))\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.subplot(2,2,1)\n",
    "        plt.imshow(frame_vq_1, cmap=plt.get_cmap('jet'))\n",
    "        plt.subplot(2,2,2)\n",
    "        plt.imshow(frame_vq_2, cmap=plt.get_cmap('jet'))\n",
    "\n",
    "        plt.subplot(2,2,3)\n",
    "        plt.imshow(np.array(frame1))\n",
    "\n",
    "        plt.subplot(2,2,4)\n",
    "        plt.imshow(np.array(frame2))\n",
    "\n",
    "\n",
    "def visualize_correspondence(z1_q, z2_q, sample_idx, frame1, frame2, scale=32):\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "\n",
    "    z1_q = z1_q[0].numpy()\n",
    "    z2_q = z2_q[0].numpy()\n",
    "    find = False\n",
    "    count = 0\n",
    "\n",
    "    while not find:\n",
    "        x, y = sample_idx % scale, sample_idx // scale\n",
    "\n",
    "        query = z1_q[y,x]\n",
    "        m = (z2_q == query).astype(np.uint8) * 255\n",
    "        count += 1\n",
    "\n",
    "        if m.max() > 1:\n",
    "            find = True\n",
    "        else:\n",
    "            # sample_idx = random.randint(0, scale*scale -1)\n",
    "            sample_idx = random.randint(210, 250)\n",
    "            print('not find, change query')\n",
    "        \n",
    "    print(f\"find correspodence at {count}\")   \n",
    "\n",
    "    querys_map = np.zeros((scale,scale))\n",
    "    querys_map[y,x] = 255\n",
    "    querys_map = querys_map.astype(np.uint8)\n",
    "    \n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(2,2,1)\n",
    "    plt.imshow(querys_map, cmap=plt.get_cmap('jet'))\n",
    "    plt.subplot(2,2,2)\n",
    "    plt.imshow(m, cmap=plt.get_cmap('jet'))\n",
    "    plt.subplot(2,2,3)\n",
    "    plt.imshow(np.array(frame1))\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.imshow(np.array(frame2))\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample frame\n",
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "samples = load_anno()\n",
    "\n",
    "video_idx = random.randint(0, len(samples) - 1)\n",
    "frame_idx = random.randint(0, samples[video_idx]['num_frames'] - 2)\n",
    "sample = samples[video_idx]\n",
    "frame1 = Image.open(sample['frames_path'][frame_idx]).convert('RGB')\n",
    "frame2 = Image.open(sample['frames_path'][frame_idx+1]).convert('RGB')\n",
    "print('sample frames from {}'.format(sample['frames_path'][0]))\n",
    "\n",
    "x1 = preprocess(frame1)\n",
    "x2 = preprocess(frame2)\n",
    "\n",
    "from IPython.display import display, display_markdown\n",
    "\n",
    "frame1 = T.ToPILImage(mode='RGB')(x1[0])\n",
    "frame2 = T.ToPILImage(mode='RGB')(x2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode for origin vqvae\n",
    "model = VQVAE(downsample=4, n_embed=16, channel=256, n_res_channel=128, embed_dim=128)\n",
    "\n",
    "model.load_state_dict(torch.load('/home/lr/models/vqvae/vqvae_youtube_d4_n16_c256_embc128_withbbox.pth')['state_dict'])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "_, quant1, diff, ind1, embed = model.encode(x1.cuda())\n",
    "_, quant2, diff, ind2, embed = model.encode(x2.cuda())\n",
    "\n",
    "print(len(np.unique(ind1.cpu().numpy())))\n",
    "\n",
    "dec1 = model.decode(quant1)\n",
    "dec2 = model.decode(quant2)\n",
    "dec1 = T.ToPILImage(mode='RGB')(dec1[0].cpu())\n",
    "dec2 = T.ToPILImage(mode='RGB')(dec2[0].cpu())\n",
    "\n",
    "visualize_vqvae(ind1.cpu(), ind2.cpu(), frame1, frame2, dec1, dec2, rescale=False)\n",
    "# visualize corr for origin vqvae\n",
    "# visualize_correspondence(ind1.cpu(), ind2.cpu(), sample_idx, frame1, frame2, scale=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and decode for origin vqvae\n",
    "model = VQVAE(downsample=4, n_embed=2048, channel=512, n_res_channel=128, embed_dim=512, newed=True)\n",
    "\n",
    "model.load_state_dict(torch.load('/home/lr/models/vqvae/vqvae_youtube_d4_n2048_c512_embc512.pth'))\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "_, quant1, diff, ind1, embed = model.encode(x1.cuda())\n",
    "_, quant2, diff, ind2, embed = model.encode(x2.cuda())\n",
    "\n",
    "print(len(np.unique(ind1.cpu().numpy())))\n",
    "\n",
    "dec1 = model.decode(quant1)\n",
    "dec2 = model.decode(quant2)\n",
    "dec1 = T.ToPILImage(mode='RGB')(dec1[0].cpu())\n",
    "dec2 = T.ToPILImage(mode='RGB')(dec2[0].cpu())\n",
    "\n",
    "visualize_vqvae(ind1.cpu(), ind2.cpu(), frame1, frame2, dec1, dec2, rescale=True)\n",
    "# visualize corr for origin vqvae\n",
    "# visualize_correspondence(ind1.cpu(), ind2.cpu(), sample_idx, frame1, frame2, scale=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VQCL(backbone=dict(type='ResNet', depth=50, strides=(1, 2, 1, 1), out_indices=(3, )),\n",
    "    K=65536,\n",
    "    m=0.999,\n",
    "    T=0.1,\n",
    "    embed_dim=128,\n",
    "    n_embed=32,\n",
    "    commitment_cost=0.0,\n",
    "    loss=dict(type='Ce_Loss',reduction='mean'))\n",
    "\n",
    "_ = load_checkpoint(model, '/home/lr/expdir/VCL/group_vqvae_tracker/train_vqvae_video_d4_nemd32_contrastive_vfs/epoch_8.pth', map_location='cpu')\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "_, quant1, diff, ind1, embed = model.encode(x1.cuda())\n",
    "_, quant2, diff, ind2, embed = model.encode(x2.cuda())\n",
    "\n",
    "print(len(np.unique(ind1.cpu().numpy())))\n",
    "\n",
    "\n",
    "visualize_vqvae(ind1.cpu(), ind2.cpu(), frame1, frame2, None, None, nembed=32, rescale=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7c246b6cc6cfc6db104182c221d279513df2c5a30140b74c2713c1b2d72f533"
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit ('970': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
