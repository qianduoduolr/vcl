{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import _init_paths\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dall_e  import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "from vcl.models.vqvae import *\n",
    "from vcl.models.trackers import *\n",
    "from vcl.utils import *\n",
    "from mmcv.runner import get_dist_info, init_dist, load_checkpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "target_image_size = 256\n",
    "step = 3\n",
    "\n",
    "output_dir = '/home/lr/project/vcl_output/vis_correspondence_ablations_local_step_3'\n",
    "\n",
    "if os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "samples = []\n",
    "list_path = '/home/lr/dataset/YouTube-VOS/2018/train/youtube2018_train.json'\n",
    "video_dir = '/home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256'\n",
    "mask_dir = '/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256'\n",
    "data = mmcv.load(list_path)\n",
    "for vname, frames in data.items():\n",
    "    sample = dict()\n",
    "    sample['frames_path'] = []\n",
    "    sample['masks_path'] = []\n",
    "    for frame in frames:\n",
    "        sample['frames_path'].append(osp.join(video_dir, vname, frame))\n",
    "        sample['masks_path'].append(osp.join(mask_dir, vname, frame.replace('jpg','png')))\n",
    "        \n",
    "    sample['num_frames'] = len(sample['frames_path'])\n",
    "\n",
    "    samples.append(sample)\n",
    "        \n",
    "visualizer = Correspondence_Visualizer(mode='pair', show_mode='none', radius=6, blend_color='jet')\n",
    "\n",
    "\n",
    "def build_model(strides=(1,2,2,1), depth=18, pretrained=None, model_pre=None, pool_type='none'):\n",
    "    # final model\n",
    "    model = VanillaTracker(\n",
    "        backbone=dict(type='ResNet', depth=depth, strides=strides, out_indices=(2, ), pool_type=pool_type, pretrained=model_pre),\n",
    "        test_cfg=dict(),\n",
    "        train_cfg=dict()\n",
    "        )\n",
    "\n",
    "    if pretrained is not None:\n",
    "        params = torch.load(pretrained)\n",
    "        state_dict = params['state_dict']\n",
    "        state_dict = { k:v for k,v in state_dict.items() if k.find('backbone') != -1}\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "    model_name = 'final'\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "\n",
    "def main(x1, x2, model, model_name='final'):\n",
    "    model.eval()\n",
    "    enc1 = model.backbone(x1.cuda())\n",
    "    enc2 = model.backbone(x2.cuda())\n",
    "    plt, f1, f2, result, query = visualizer.visualize([frame1, frame2], [enc1, enc2], sample_idx, return_all=True)\n",
    "\n",
    "    \n",
    "    os.makedirs(os.path.join(output_dir,model_name, video_name), exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir,model_name,video_name, video_name+'.png'))\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'target.jpg'), f1)\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'ref.jpg'), f2)\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'result.jpg'), result)\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'query.jpg'), query)\n",
    "    \n",
    "    \n",
    "    out = np.concatenate([query, result], 1)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:57: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:57: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "2022-05-16 17:25:56,784 - vcl - INFO - Loading /home/lr/models/ssl/image_based/moco_v2_res18_ep200_lab.pth as torchvision\n",
      "2022-05-16 17:25:56,801 - vcl - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.0.weight', 'fc.2.weight', 'fc.2.bias', 'fc.0.bias'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/lr/models/ssl/image_based/moco_v2_res18_ep200_lab.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-16 17:25:57,081 - vcl - INFO - Loading /home/lr/models/ssl/image_based/detco_200ep_AA.pth as torchvision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/lr/models/ssl/image_based/detco_200ep_AA.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3549/3813139623.py:57: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if model_name is 'detco':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/5da6b2dc5d/00055.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/5da6b2dc5d/00055.jpg 0\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/8aad0591eb/00105.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/8aad0591eb/00105.jpg 1\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/e8962324e3/00110.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/e8962324e3/00110.jpg 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3549/3813139623.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2_lab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_3549/300900433.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(x1, x2, model, model_name)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0menc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0menc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0menc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/vcl/vcl/utils/visualize.py\u001b[0m in \u001b[0;36mvisualize\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'pair'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mplt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_pairwise_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vq'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mplt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_vq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/vcl/vcl/utils/visualize.py\u001b[0m in \u001b[0;36mvis_pairwise_attention\u001b[0;34m(self, frames, fs, sample_idx, return_all)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffanity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0matt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0matt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## sample frame\n",
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "vname = None\n",
    "\n",
    "model1 = build_model(model_pre='/home/lr/models/ssl/image_based/moco_v2_res18_ep200_lab.pth')\n",
    "model2 = build_model(strides=(1,2,1,1), depth=50, model_pre='/home/lr/models/ssl/image_based/detco_200ep_AA.pth', pool_type='max')\n",
    "model3 = build_model(pretrained='/home/lr/expdir/VCL/group_vqvae_tracker/mast_d4_l2_base_5/epoch_1600.pth')\n",
    "model4 = build_model(pretrained='/home/lr/expdir/VCL/group_vqvae_tracker/mast_d4_l2_pyramid_dis_16/epoch_3200.pth')\n",
    "model5 = build_model(depth=50, pretrained='/home/lr/expdir/VCL/group_vqvae_tracker/final_framework_v2_9/epoch_1600.pth')\n",
    "\n",
    "models = [ model1, model2, model3, model4, model5 ]\n",
    "\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "\n",
    "    num_frames = sample['num_frames']\n",
    "    frame_idx = random.randint(0, num_frames-step-1)\n",
    "\n",
    "    video_name = sample['frames_path'][0].split('/')[-2]\n",
    "\n",
    "\n",
    "    frame1 = cv2.imread(sample['frames_path'][frame_idx])[:,:,::-1]\n",
    "\n",
    "    frame2 = cv2.imread(sample['frames_path'][min(frame_idx+step, len(sample['frames_path'])-1)])[:,:,::-1]\n",
    "    \n",
    "    frame1 = cv2.resize(frame1, (256,256))\n",
    "    frame2 = cv2.resize(frame2, (256,256))\n",
    "    \n",
    "    print(sample['masks_path'][frame_idx])\n",
    "    mask = mmcv.imread(sample['masks_path'][frame_idx], flag='unchanged', backend='pillow')\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    \n",
    "    m = mmcv.imresize(mask, (32,32), interpolation='nearest').reshape(-1)\n",
    "    \n",
    "    idxs = np.nonzero(m)[0].tolist()\n",
    "    \n",
    "    if len(idxs) > 0:\n",
    "        sample_idx = random.choice(idxs)\n",
    "    else:\n",
    "        sample_idx = random.randint(0, 32 ** 2 -1)\n",
    "    # print(idxs, sample_idx)\n",
    "\n",
    "    print('sample frames from {}'.format(sample['frames_path'][frame_idx]), i)\n",
    "\n",
    "    x1_rgb = preprocess_(frame1, mode='rgb')\n",
    "    x2_rgb = preprocess_(frame2, mode='rgb')\n",
    "\n",
    "    x1_lab = preprocess_(frame1, mode='lab')\n",
    "    x2_lab = preprocess_(frame2, mode='lab')\n",
    "    \n",
    "    outs = []\n",
    "    model_names = ['moco_v2_lab_res18', 'detco', 'rec', 'temporal', 'final']\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if model_name is 'detco':\n",
    "            out = main(x1_rgb, x2_rgb, models[idx], model_name=model_name)\n",
    "        else:\n",
    "            out = main(x1_lab, x2_lab, models[idx], model_name=model_name)\n",
    "            \n",
    "        outs.append(out)\n",
    "        result = np.concatenate(outs, 0)\n",
    "        cv2.imwrite(osp.join(output_dir, video_name)+'.jpg', result)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f8e44a6ef544108172ff7c3d1cb2d54c99af7e07cfc4748358316ff08241482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
