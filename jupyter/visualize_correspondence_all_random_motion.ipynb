{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import _init_paths\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from dall_e  import map_pixels, unmap_pixels, load_model\n",
    "from IPython.display import display, display_markdown\n",
    "from vcl.models.vqvae import *\n",
    "from vcl.models.trackers import *\n",
    "from vcl.utils import *\n",
    "from mmcv.runner import get_dist_info, init_dist, load_checkpoint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "target_image_size = 256\n",
    "step = 10\n",
    "\n",
    "output_dir = f'/home/lr/project/vcl_output/vis_correspondence_ablations_local_step_{step}_flow'\n",
    "\n",
    "if os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "samples = []\n",
    "list_path = '/home/lr/dataset/YouTube-VOS/2018/train/ytvos_s256_flow_raft.json'\n",
    "video_dir = '/home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256'\n",
    "mask_dir = '/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256'\n",
    "data = mmcv.load(list_path)['train']\n",
    "\n",
    "for vid in data:\n",
    "    sample = dict()\n",
    "    vname = vid[\"base_path\"].split('/')[-1]\n",
    "    sample['frames_path'] = []\n",
    "    sample['frames_bbox'] = []\n",
    "    sample['masks_path'] = []\n",
    "    sample['num_frames'] = len(vid['frame'])\n",
    "\n",
    "    for frame in vid['frame']:\n",
    "        sample['frames_path'].append(osp.join(video_dir, vname, frame['img_path']))\n",
    "        sample['frames_bbox'].append(frame['objs'][0]['bbox'])\n",
    "        sample['masks_path'].append(osp.join(mask_dir, vname, frame['img_path'].replace('jpg','png')))\n",
    "        \n",
    "    if sample['num_frames'] <= step:\n",
    "        continue\n",
    "\n",
    "    samples.append(sample)\n",
    "        \n",
    "visualizer = Correspondence_Visualizer(mode='pair', show_mode='none', radius=6, blend_color='jet')\n",
    "\n",
    "\n",
    "def build_model(strides=(1,2,2,1), depth=18, pretrained=None, model_pre=None, pool_type='none'):\n",
    "    # final model\n",
    "    model = VanillaTracker(\n",
    "        backbone=dict(type='ResNet', depth=depth, strides=strides, out_indices=(2, ), pool_type=pool_type, pretrained=model_pre),\n",
    "        test_cfg=dict(),\n",
    "        train_cfg=dict()\n",
    "        )\n",
    "\n",
    "    if pretrained is not None:\n",
    "        params = torch.load(pretrained)\n",
    "        state_dict = params['state_dict']\n",
    "        state_dict = { k:v for k,v in state_dict.items() if k.find('backbone') != -1}\n",
    "        model.load_state_dict(state_dict, strict=False)\n",
    "        \n",
    "    model_name = 'final'\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "    \n",
    "\n",
    "def main(x1, x2, model, model_name='final'):\n",
    "    model.eval()\n",
    "    enc1 = model.backbone(x1.cuda())\n",
    "    enc2 = model.backbone(x2.cuda())\n",
    "    plt, f1, f2, result, query = visualizer.visualize([frame1, frame2], [enc1, enc2], sample_idx, return_all=True)\n",
    "\n",
    "    \n",
    "    os.makedirs(os.path.join(output_dir,model_name, video_name), exist_ok=True)\n",
    "    plt.savefig(os.path.join(output_dir,model_name,video_name, video_name+'.png'))\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'target.jpg'), f1)\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'ref.jpg'), f2)\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'result.jpg'), result)\n",
    "    cv2.imwrite(os.path.join(output_dir,model_name,video_name, 'query.jpg'), query)\n",
    "    \n",
    "    \n",
    "    out = np.concatenate([query, result], 1)\n",
    "    return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:66: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:66: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "2022-05-17 12:48:38,982 - vcl - INFO - Loading /home/lr/models/ssl/image_based/moco_v2_res18_ep200_lab.pth as torchvision\n",
      "2022-05-17 12:48:39,004 - vcl - INFO - These parameters in pretrained checkpoint are not loaded: {'fc.0.bias', 'fc.2.weight', 'fc.2.bias', 'fc.0.weight'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/lr/models/ssl/image_based/moco_v2_res18_ep200_lab.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-17 12:48:39,306 - vcl - INFO - Loading /home/lr/models/ssl/image_based/detco_200ep_AA.pth as torchvision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/lr/models/ssl/image_based/detco_200ep_AA.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22163/1323717906.py:66: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if model_name is 'detco':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/003234408d/00105.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/003234408d/00105.jpg 0\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/0043f083b5/00015.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/0043f083b5/00015.jpg 1\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/0044fa5fba/00030.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/0044fa5fba/00030.jpg 2\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/005a527edd/00055.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/005a527edd/00055.jpg 3\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/0065b171f9/00000.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/0065b171f9/00000.jpg 4\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/00917dcfc4/00045.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/00917dcfc4/00045.jpg 5\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/00a23ccf53/00085.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/00a23ccf53/00085.jpg 6\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/00ad5016a4/00020.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/00ad5016a4/00020.jpg 7\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01082ae388/00080.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01082ae388/00080.jpg 8\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/011ac0a06f/00015.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/011ac0a06f/00015.jpg 9\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/013099c098/00005.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/013099c098/00005.jpg 10\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/0155498c85/00015.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/0155498c85/00015.jpg 11\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01694ad9c8/00030.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01694ad9c8/00030.jpg 12\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/017ac35701/00120.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/017ac35701/00120.jpg 13\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01b80e8e1a/00000.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01b80e8e1a/00000.jpg 14\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01baa5a4e1/00090.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01baa5a4e1/00090.jpg 15\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01c3111683/00040.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01c3111683/00040.jpg 16\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01c4cb5ffe/00010.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01c4cb5ffe/00010.jpg 17\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01c76f0a82/00045.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01c76f0a82/00045.jpg 18\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01c783268c/00080.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01c783268c/00080.jpg 19\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01e64dd36a/00000.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01e64dd36a/00000.jpg 20\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01ed275c6e/00020.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01ed275c6e/00020.jpg 21\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/01ff60d1fa/00065.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/01ff60d1fa/00065.jpg 22\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/020cd28cd2/00015.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/020cd28cd2/00015.jpg 23\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/02264db755/00050.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/02264db755/00050.jpg 24\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/0248626d9a/00005.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/0248626d9a/00005.jpg 25\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/02668dbffa/00055.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/02668dbffa/00055.jpg 26\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/0274193026/00075.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/0274193026/00075.jpg 27\n",
      "/home/lr/dataset/YouTube-VOS/2018/train/Annotations_s256/02d28375aa/00040.png\n",
      "sample frames from /home/lr/dataset/YouTube-VOS/2018/train/JPEGImages_s256/02d28375aa/00040.jpg 28\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty range for randrange() (0, 0, 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22163/1323717906.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mnum_frames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_frames'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mframe_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mvideo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'frames_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandint\u001b[0;34m(self, a, b)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \"\"\"\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_randbelow_with_getrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/random.py\u001b[0m in \u001b[0;36mrandrange\u001b[0;34m(self, start, stop, step, _int)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mistart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_randbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"empty range for randrange() (%d, %d, %d)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mistart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;31m# Non-unit step argument supplied.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: empty range for randrange() (0, 0, 0)"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## sample frame\n",
    "import os.path as osp\n",
    "import glob\n",
    "\n",
    "vname = None\n",
    "\n",
    "model1 = build_model(model_pre='/home/lr/models/ssl/image_based/moco_v2_res18_ep200_lab.pth')\n",
    "model2 = build_model(strides=(1,2,1,1), depth=50, model_pre='/home/lr/models/ssl/image_based/detco_200ep_AA.pth', pool_type='max')\n",
    "model3 = build_model(pretrained='/home/lr/expdir/VCL/group_vqvae_tracker/mast_d4_l2_base_5/epoch_1600.pth')\n",
    "model4 = build_model(pretrained='/home/lr/expdir/VCL/group_vqvae_tracker/mast_d4_l2_pyramid_dis_16/epoch_3200.pth')\n",
    "model5 = build_model(depth=50, pretrained='/home/lr/expdir/VCL/group_vqvae_tracker/final_framework_v2_9/epoch_1600.pth')\n",
    "\n",
    "models = [ model1, model2, model3, model4, model5 ]\n",
    "\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "\n",
    "    num_frames = sample['num_frames']\n",
    "    frame_idx = random.randint(0, num_frames-step-1)\n",
    "\n",
    "    video_name = sample['frames_path'][0].split('/')[-2]\n",
    "\n",
    "\n",
    "    frame1 = cv2.imread(sample['frames_path'][frame_idx])[:,:,::-1]\n",
    "\n",
    "    frame2 = cv2.imread(sample['frames_path'][min(frame_idx+step, len(sample['frames_path'])-1)])[:,:,::-1]\n",
    "    \n",
    "    frame1 = cv2.resize(frame1, (256,256))\n",
    "    frame2 = cv2.resize(frame2, (256,256))\n",
    "    \n",
    "\n",
    "    \n",
    "    print(sample['masks_path'][frame_idx])\n",
    "    mask = mmcv.imread(sample['masks_path'][frame_idx], flag='unchanged', backend='pillow')\n",
    "\n",
    "    bbox = sample['frames_bbox'][frame_idx]\n",
    "    bbox_mask = np.zeros(mask.shape)\n",
    "    bbox_mask[bbox[1]:bbox[3]+1, bbox[0]:bbox[2]+1] = 1\n",
    "    \n",
    "    mask = (mask > 0).astype(np.uint8) \n",
    "    mask = (mask * bbox_mask).astype(np.uint8)\n",
    "    \n",
    "    \n",
    "    m = mmcv.imresize(mask, (32,32), interpolation='nearest').reshape(-1)\n",
    "    \n",
    "    idxs = np.nonzero(m)[0].tolist()\n",
    "    \n",
    "    if len(idxs) > 0:\n",
    "        sample_idx = random.choice(idxs)\n",
    "    else:\n",
    "        sample_idx = random.randint(0, 32 ** 2 -1)\n",
    "    # print(idxs, sample_idx)\n",
    "\n",
    "    print('sample frames from {}'.format(sample['frames_path'][frame_idx]), i)\n",
    "\n",
    "    x1_rgb = preprocess_(frame1, mode='rgb')\n",
    "    x2_rgb = preprocess_(frame2, mode='rgb')\n",
    "\n",
    "    x1_lab = preprocess_(frame1, mode='lab')\n",
    "    x2_lab = preprocess_(frame2, mode='lab')\n",
    "    \n",
    "    outs = []\n",
    "    model_names = ['moco_v2_lab_res18', 'detco', 'rec', 'temporal', 'final']\n",
    "    \n",
    "    for idx, model_name in enumerate(model_names):\n",
    "        if model_name is 'detco':\n",
    "            out = main(x1_rgb, x2_rgb, models[idx], model_name=model_name)\n",
    "        else:\n",
    "            out = main(x1_lab, x2_lab, models[idx], model_name=model_name)\n",
    "            \n",
    "        outs.append(out)\n",
    "        result = np.concatenate(outs, 0)\n",
    "        cv2.imwrite(osp.join(output_dir, video_name)+'.jpg', result)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f8e44a6ef544108172ff7c3d1cb2d54c99af7e07cfc4748358316ff08241482"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
